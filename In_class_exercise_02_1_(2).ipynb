{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON242IzMKH6O"
      },
      "source": [
        "## The third In-class-exercise (02/08/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m00H5hnWKH6P"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcL4NUQmKH6P"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTLw1dISKH6Q"
      },
      "outputs": [],
      "source": [
        "Rate of Co-vid 19 recovery?\n",
        "Even though the virus is life threatening, people have managed to recover from covid infection and continue to live normal healthy lives.\n",
        "Although most covid infected patience die of virus, a small significant number of patience beat the infection. This significant statistics\n",
        "can be obtained through collecting of general population data, total tested persons data, total infected persons data and data for total\n",
        "persons killed by the virus. Four sets of data are required which include general population,tested persons data,death toll data \n",
        "and data for the total number of people infected by the virus. The data can be collected visiting the \"https://www.worldometers.info/coronavirus/\" \n",
        "site and easily getting the numbers for each set of data required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKbmgZG8KH6Q"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ME1YboXKH6R"
      },
      "outputs": [],
      "source": [
        "gen_population = list(map(int,input(\"Enter number of total population\")))\n",
        "tested = list(map(int,input(\"Enter number of total tested persons\")))\n",
        "infected = list(map(int,input(\"Enter total number of infected persons\")))\n",
        "death_toll = list(map(int,input(\"Enter the total number of deaths\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQX-dhiKH6R"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CoTwoE9KH6S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "from selenium import webdriver\n",
        "from BeautifulSoup import BeautifulSoup\n",
        "\n",
        "driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
        "\n",
        "title = [] \n",
        "published = [] \n",
        "year = []\n",
        "authors = []\n",
        "abstract = []\n",
        "driver.get(\"https://scholar.google.com/\")\n",
        "conect = driver.page_source\n",
        "soup = BeautifulSoup(content)\n",
        "for a in soup.findAll('a',href=True, attrs={'class':'gs_a'}):\n",
        "    title = a.find('div', attrs={'class':''})\n",
        "    published = a.find('div', attrs={'class':'gs_a'})\n",
        "    year = a.find('div', attrs={'class':'gs_a'})\n",
        "    authors = a.find('div', attrs={'class':'gs_a'})\n",
        "    abstract = a.find('div', attrs={'class':'gs_rs'})\n",
        "\n",
        "df = pd.DataFrame({'Title':title, 'Journal':published, 'Year':year, 'Authors':authors, 'Abstract':abstract})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUx9eZy5KH6S"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWqmRGIbKH6T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from BeautifulSoup import BeautifulSoup\n",
        "\n",
        "driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
        "\n",
        "user_name = []\n",
        "posted_time = []\n",
        "text = []\n",
        "driver.get(\"https://twitter.com/?lang=en\")\n",
        "conect = driver.page_source\n",
        "soup = BeautifulSoup(content)\n",
        "for a in soup.findAll('a',href=True, attrs={'class':''}):\n",
        "    user_name = a.find('div', attrs={'class':''})\n",
        "    posted_time = a.find('div', attrs={'class':''})\n",
        "    text = a.find('div', attrs={'class':''})\n",
        "    \n",
        "df = pd.DataFrame({'User_name':user_name, 'Posted_time':posted_time, 'Text':text})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "In-class-exercise-02-1 (2).ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}